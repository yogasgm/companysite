{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1wlIg5JpgK5Z/ZGtN90LN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yogasgm/companysite/blob/main/practice_material/004_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Topic Modeling**"
      ],
      "metadata": {
        "id": "UByqXBpxf551"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing required libraries**"
      ],
      "metadata": {
        "id": "Af_ZEEXvYqjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbsoS6qw9RVp"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis --no-deps --quiet\n",
        "!pip install funcy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mn7AB6w5z1r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import gensim\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "import pyLDAvis.gensim_models\n",
        "import warnings\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing Dataset**"
      ],
      "metadata": {
        "id": "OWmOaOF4WDR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the dataset from GitHub\n",
        "data_url = \"https://raw.githubusercontent.com/andrybrew/IHT-SEM1302-30Okt/main/data/001_suku-bunga.csv\"\n",
        "\n",
        "# Using pandas read_csv function to load the data from the URL directly into a DataFrame\n",
        "df_tweet = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "jZpa9wOZWMdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3jScZiTEjf"
      },
      "source": [
        "##**Data Preprocessing for Topic Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for removing URL\n",
        "def remove_url(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+|\\S+\\.\\S+/\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "# Remove URL from each tweet\n",
        "df_tweet['text'] = df_tweet['text'].apply(remove_url)\n",
        "\n",
        "# Remove mentions entirely\n",
        "df_tweet['text'] = df_tweet['text'].str.replace(r'@\\S+', '', regex=True)\n",
        "\n",
        "# Remove non-word characters except for spaces and %\n",
        "df_tweet['text'] = df_tweet['text'].str.replace(r'[^\\w\\s%]', '', regex=True)\n",
        "\n",
        "# Convert to lowercase\n",
        "df_tweet['text'] = df_tweet['text'].str.lower()\n",
        "\n",
        "# Trim leading and trailing spaces and replace multiple spaces with a single space\n",
        "df_tweet['text'] = df_tweet['text'].str.strip().str.replace(r'\\s+', ' ', regex=True)"
      ],
      "metadata": {
        "id": "T6JGsyHI2K2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g4Qia2Q98uS"
      },
      "outputs": [],
      "source": [
        "# Select Tweets\n",
        "tweet = df_tweet['text']\n",
        "\n",
        "# Tokenize\n",
        "tokenized_text = [d.lower().split() for d in tweet]\n",
        "\n",
        "# Remove punctuation\n",
        "punctuation = string.punctuation\n",
        "tokenized_text = [[word for word in doc if word not in punctuation] for doc in tokenized_text]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenized_text = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokenized_text]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = stopwords.words('indonesian')\n",
        "tokenized_text = [[word for word in doc if word not in stop_words] for doc in tokenized_text]\n",
        "\n",
        "# Adding additional words to the stop words list\n",
        "custom_stop_words = ['dgn', 'sdh', 'yg', 'the', 'gak', 'ga', 'a', 'krn', 'thd', 'nya', 'ya', 'n', 'kalo', 'aja', 'deh', 'tuh', 'udah', 'dll.', '2', '25', '20', '1.', '2.', '7.', 'u', '5', 'gua', 'â€¢']\n",
        "stop_words.extend(custom_stop_words)\n",
        "\n",
        "# Remove stopwords again\n",
        "tokenized_text = [[word for word in doc if word not in stop_words] for doc in tokenized_text]\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(tokenized_text)\n",
        "\n",
        "# Create corpus\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_text]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setting Up LDA Model**"
      ],
      "metadata": {
        "id": "PZm36XHPec_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53FC4XGg9fXE"
      },
      "outputs": [],
      "source": [
        "# Train LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics= 3,\n",
        "                                           passes = 22,\n",
        "                                           per_word_topics=True,\n",
        "                                           random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Visualizing Topics**"
      ],
      "metadata": {
        "id": "lw-JYt9PehXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVqwRX_s9-iq"
      },
      "outputs": [],
      "source": [
        "# Enable Notebook\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# Visualize\n",
        "pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the best topics\n",
        "top_topics = lda_model.print_topics(num_words=10)  # Display the top 10 keywords for each topics\n",
        "\n",
        "# Create DataFrame\n",
        "df_topics = pd.DataFrame(top_topics, columns=['Topic', 'Keywords'])\n",
        "\n",
        "# Set topic as index\n",
        "df_topics.set_index('Topic', inplace=True)\n",
        "\n",
        "# Show df_topics\n",
        "df_topics"
      ],
      "metadata": {
        "id": "E7m2XR0g7EJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topik 0:**\n",
        "\n",
        "**Kata Kunci Utama:** bunga, suku, turun, kenaikan, fed, inflasi, saham, bank, masyarakat, ekonomi.\n",
        "\n",
        "**Interpretasi:** Membahas dampak naik turunnya suku bunga terutama dari bank sentral (seperti Fed) terhadap ekonomi, pasar saham, dan inflasi.\n",
        "\n",
        "**Topik 1:**\n",
        "\n",
        "**Kata Kunci Utama:** bunga, suku, bi, acuan, 6%, rupiah, bank, indonesia, persen, naikkan.\n",
        "\n",
        "**Interpretasi:** Fokus pada kebijakan suku bunga Bank Indonesia, efeknya pada nilai rupiah, dan ekonomi Indonesia.\n",
        "\n",
        "**Topik 2:**\n",
        "\n",
        "**Kata Kunci Utama:** bunga, suku, bank, fed, bi, turun, kenaikan, negara, inflasi, menaikkan.\n",
        "\n",
        "**Interpretasi:** Berbicara tentang kebijakan suku bunga oleh bank-bank central seperti Federal Reserve dan Bank Indonesia, serta efeknya terhadap inflasi dan ekonomi global dan nasional."
      ],
      "metadata": {
        "id": "XFcGYQCx7rjk"
      }
    }
  ]
}